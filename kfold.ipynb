{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alilim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2022-11-13 22:57:15.822639: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-13 22:57:16.104699: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-13 22:57:16.182120: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-13 22:57:17.540985: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-13 22:57:17.541234: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-13 22:57:17.541251: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Embedding, TimeDistributed, Bidirectional,GlobalMaxPooling1D\n",
    "\n",
    "\n",
    "from tcdf_text_classification.iob_transformer import iob_transformer\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "from livelossplot.tf_keras import PlotLossesCallback\n",
    "\n",
    "\n",
    "from seqeval.metrics import f1_score, classification_report, precision_score, recall_score\n",
    "\n",
    "from plot_keras_history import plot_history\n",
    "\n",
    "from tf2crf import CRF, ModelWithCRFLoss, ModelWithCRFLossDSCLoss\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('DODFCorpus_contratos_licitacoes_v2.csv')\n",
    "\n",
    "df = df.drop(['Unnamed: 0','Unnamed: 0.1'], axis =1)\n",
    "\n",
    "#regex pra resolver esse tipo de problema -> Processo:0...\n",
    "df['texto'] = df['texto'].str.replace(r'([A-Za-z]:)[0-9]', r'\\1 ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.query(\"tipo_rel == 'REL_ANUL_REVOG_LICITACAO'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['REL_ANUL_REVOG_LICITACAO'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tipo_rel.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOB, transformação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_wrong_tags(label_list):\n",
    "  for label in label_list:\n",
    "    for idx,w in enumerate(label):\n",
    "      if w in ['B-11','B-12','B-50']:\n",
    "        label[idx] = 'O'\n",
    "\n",
    "\n",
    "def get_uniquev(acts,labels):\n",
    "\n",
    "  #salvando todas as palavras do corpus sem repetição\n",
    "  words = set()\n",
    "\n",
    "  for act in acts:\n",
    "    for word in act:\n",
    "      words.add(word)\n",
    "  #convertendo o set em uma lista\n",
    "  words = list(words)\n",
    "\n",
    "  words.append(\"ENDPAD\")\n",
    "  words.append(\"UNK\")\n",
    "\n",
    "  words_amt = len(words)\n",
    "\n",
    "  tags = set()\n",
    "\n",
    "  for label in labels:\n",
    "    for tag in label:\n",
    "      tags.add(tag)\n",
    "\n",
    "  tags = list(tags)\n",
    "  tags_amt = len(tags)\n",
    "\n",
    "  return words, tags, words_amt, tags_amt\n",
    "\n",
    "\n",
    "def get_dicts(words, tags):\n",
    "\n",
    "  lab_enc = LabelEncoder()\n",
    "\n",
    "  lab_enc.fit(words)\n",
    "  words_i = dict(zip(lab_enc.classes_, lab_enc.transform(lab_enc.classes_)))\n",
    "\n",
    "  i_words = {}\n",
    "\n",
    "  for key in words_i:\n",
    "    i_words[words_i[key]] = key\n",
    "\n",
    "  lab_enc = LabelEncoder()\n",
    "\n",
    "  lab_enc.fit(tags)\n",
    "  tags_i = dict(zip(lab_enc.classes_, lab_enc.transform(lab_enc.classes_)))\n",
    "\n",
    "\n",
    "  i_tags = {}\n",
    "\n",
    "  for key in tags_i:\n",
    "    i_tags[tags_i[key]] = key\n",
    "\n",
    "\n",
    "  return words_i, i_words, tags_i, i_tags\n",
    "\n",
    "\n",
    "def transform_data(x, y, tags_i, words_i):\n",
    "  X,Y = [],[]\n",
    "\n",
    "  for act in x:\n",
    "    aux = []\n",
    "    for word in act:\n",
    "      aux.append(words_i[word])\n",
    "    X.append(aux)\n",
    "\n",
    "  for label in y:\n",
    "    aux = []\n",
    "    for word in label:\n",
    "      aux.append(tags_i[word])\n",
    "    Y.append(aux)\n",
    "\n",
    "  return X,Y\n",
    "\n",
    "\n",
    "\n",
    "def handle_data(dataf):\n",
    "\n",
    "  data_info = {}\n",
    "\n",
    "  iob = iob_transformer('id_ato', 'texto', 'tipo_ent', keep_punctuation=True, return_df=False)\n",
    "  iob_data = iob_transformer('id_ato', 'texto', 'tipo_ent', keep_punctuation=True, return_df=True)\n",
    "\n",
    "  acts, labels = iob.transform(data)\n",
    "\n",
    "  remove_wrong_tags(labels)\n",
    "  \n",
    "  df_iob = iob_data.transform(dataf)\n",
    "\n",
    "\n",
    "  df_iob.loc[df_iob.Word == 'B-11','Tag']='O'\n",
    "  df_iob.loc[df_iob.Word == 'B-12','Tag']='O'\n",
    "  df_iob.loc[df_iob.Word == 'B-50','Tag']='O'\n",
    "\n",
    "  \n",
    "  data_info['words'], data_info['tags'], data_info['words_amt'], data_info['tags_amt'] = get_uniquev(acts, labels)\n",
    "\n",
    "\n",
    "  vocab = {}\n",
    "\n",
    "  for i in range(0,len(acts)):\n",
    "    for word in acts[i]:\n",
    "      if word.lower() not in vocab:\n",
    "        vocab[word.lower()] = 1\n",
    "      else:\n",
    "        vocab[word.lower()]+=1\n",
    "\n",
    "  data_info['words_i'], data_info['i_words'], data_info['tags_i'], data_info['i_tags'] = get_dicts(data_info['words'],data_info['tags'])\n",
    "\n",
    "  inputs,targets = transform_data(acts,labels, data_info['tags_i'], data_info['words_i'])\n",
    "\n",
    "  inputs = pad_sequences(maxlen=max_length, sequences=inputs, padding=\"post\", value=data_info['words_i']['ENDPAD'])\n",
    "  targets = pad_sequences(maxlen=max_length, sequences=targets, padding=\"post\", value=data_info['tags_i'][\"O\"])\n",
    "\n",
    "\n",
    "  return inputs, targets, data_info\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_values(index_array,y_test, i_tags):\n",
    "  pred_tags = []\n",
    "  real_tags = []\n",
    "\n",
    "  for act in index_array:\n",
    "    act_tags = []\n",
    "    for w in act:\n",
    "      act_tags.append(i_tags[w])\n",
    "    pred_tags.append(act_tags)\n",
    "\n",
    "  for ato in y_test:\n",
    "    tags_ato = []\n",
    "    for palavra in ato:\n",
    "      tags_ato.append(i_tags[palavra])\n",
    "    real_tags.append(tags_ato)\n",
    "\n",
    "  return real_tags, pred_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Funções auxiliares\n",
    "def calc_f1(data_info):    \n",
    "\n",
    "    def get_f1(y_true, y_pred):\n",
    "        y_pred = np.argmax(y_pred, axis=-1)\n",
    "        real_tags,pred_tags=convert_values(y_pred,y_true.numpy(),data_info['i_tags'])\n",
    "\n",
    "        f1 = f1_score(real_tags,pred_tags)\n",
    "\n",
    "        return f1\n",
    "\n",
    "    return get_f1\n",
    "\n",
    "def plotting(h,name):\n",
    "  plt.plot(h.history['get_f1'])\n",
    "  plt.plot(h.history['val_get_f1'])\n",
    "  plt.title('model f1-scores')\n",
    "  plt.ylabel('f1-score')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['training set','validation set'], loc='upper left')\n",
    "\n",
    "  plt.savefig('grafs/'+ name + '.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(data_info):\n",
    "\n",
    "    df2 =  pd.read_csv('dodf_atos_pessoal_final_version.csv')\n",
    "\n",
    "\n",
    "    l = [df2[\"texto_rel\"]]\n",
    "    headers = [\"texto\"]\n",
    "    df_emb = pd.concat(l, axis=1, keys=headers)\n",
    "\n",
    "    df_emb.drop_duplicates(subset=['texto'],inplace=True)\n",
    "\n",
    "    t_aux = list(df_emb[\"texto\"])\n",
    "    text = []\n",
    "\n",
    "    for sent in t_aux:\n",
    "        ap = sent.lower()\n",
    "        ap = word_tokenize(ap)\n",
    "        text.append(ap)\n",
    "\n",
    "    \n",
    "    model_emb = Word2Vec(min_count=1, window=5)\n",
    "    model_emb.build_vocab(text)  \n",
    "    model_emb.train(text, total_examples=model_emb.corpus_count, epochs=model_emb.epochs)\n",
    "\n",
    "    word_vectors = model_emb.wv\n",
    "    emb_dim = len(word_vectors[0])\n",
    "\n",
    "    emb_mtx = np.zeros((data_info['words_amt'], emb_dim))\n",
    "\n",
    "\n",
    "    for word, i in data_info['words_i'].items():\n",
    "        if word in word_vectors:\n",
    "            emb_vec = word_vectors[word]\n",
    "            emb_mtx[i] = emb_vec\n",
    "        else:\n",
    "            emb_mtx[i] = np.random.normal(0,1,emb_dim)\n",
    "\n",
    "    \n",
    "    import keras\n",
    "    embedding_layer = Embedding(data_info['words_amt'],\n",
    "                            emb_dim,\n",
    "                            embeddings_initializer=keras.initializers.Constant(emb_mtx),\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "\n",
    "\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(inputs, targets, data_info):\n",
    "\n",
    "    embedding_layer = word2vec(data_info)\n",
    "\n",
    "    acc = []\n",
    "    loss = []\n",
    "    f1 = []\n",
    "    reports = []\n",
    "\n",
    "    fold_no = 1\n",
    "\n",
    "\n",
    "    for train, test in kfold.split(inputs, targets):\n",
    "\n",
    "        #Modelo\n",
    "        w2v_lstm  = Sequential()\n",
    "        w2v_lstm.add(embedding_layer)\n",
    "        w2v_lstm.add(LSTM(100, return_sequences=True))\n",
    "        w2v_lstm.add(Dropout(0.5))\n",
    "        w2v_lstm.add(Dense(data_info['tags_amt'], activation=\"softmax\"))\n",
    "        w2v_lstm.summary()\n",
    "\n",
    "        adam = Adam(learning_rate=0.0095)\n",
    "\n",
    "        w2v_lstm.compile(optimizer=adam,loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='accuracy', min_delta=0, patience=5, verbose=0, mode='max', baseline=None, restore_best_weights=False)\n",
    "        callbacks = [early_stopping]\n",
    "\n",
    "\n",
    "        history = w2v_lstm.fit(inputs[train],targets[train],batch_size=12, epochs=25, callbacks=callbacks)\n",
    "\n",
    "\n",
    "        scores = w2v_lstm.evaluate(inputs[test], targets[test], verbose=0)\n",
    "        print(f'Score for fold {fold_no}: {w2v_lstm.metrics_names[0]} of {scores[0]}; {w2v_lstm.metrics_names[1]} of {scores[1]*100}%')\n",
    "        acc.append(scores[1] * 100)\n",
    "        loss.append(scores[0])\n",
    "\n",
    "\n",
    "        predictions = w2v_lstm.predict(inputs[test], verbose=0)\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "        real_tags, pred_tags = convert_values(predictions,targets[test], data_info['i_tags'])\n",
    "        f1.append(f1_score(real_tags, pred_tags))\n",
    "        \n",
    "        reports.append(classification_report(real_tags,pred_tags))\n",
    "\n",
    "        model_name = 'models/lstm_f' + str(fold_no) + '.h5'\n",
    "\n",
    "        w2v_lstm.save(model_name)\n",
    "\n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "    return acc, loss, f1, reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnlstm(inputs,targets,data_info):    \n",
    "    \n",
    "    acc = []\n",
    "    loss = []\n",
    "    f1 = []\n",
    "    reports = []\n",
    "\n",
    "    fold_no = 1\n",
    "\n",
    "    for train, test in kfold.split(inputs, targets):\n",
    "\n",
    "        #Modelo\n",
    "        cnn_lstm = Sequential()\n",
    "        cnn_lstm.add(Embedding(input_dim=data_info['words_amt']+1, output_dim=50, input_length=max_length))\n",
    "        cnn_lstm.add(Conv1D(filters=data_info['tags_amt'], kernel_size=3, padding='same', activation='relu'))\n",
    "        cnn_lstm.add(LSTM(100, return_sequences=True))\n",
    "        cnn_lstm.add(Dropout(0.5))\n",
    "        cnn_lstm.add(Dense(data_info['tags_amt'], activation='sigmoid'))\n",
    "\n",
    "        adam = Adam(learning_rate=0.009)\n",
    "\n",
    "        cnn_lstm.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='accuracy', min_delta=0, patience=5, verbose=0, mode='max', baseline=None, restore_best_weights=False)\n",
    "        callbacks = [early_stopping]\n",
    "\n",
    "        history = cnn_lstm.fit(inputs[train],targets[train],batch_size=40, epochs=20, callbacks=callbacks)\n",
    "\n",
    "\n",
    "        scores = cnn_lstm.evaluate(inputs[test], targets[test], verbose=0)\n",
    "        print(f'Score for fold {fold_no}: {cnn_lstm.metrics_names[0]} of {scores[0]}; {cnn_lstm.metrics_names[1]} of {scores[1]*100}%')\n",
    "        acc.append(scores[1] * 100)\n",
    "        loss.append(scores[0])\n",
    "\n",
    "\n",
    "        predictions = cnn_lstm.predict(inputs[test], verbose=0)\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "        real_tags, pred_tags = convert_values(predictions,targets[test],data_info['i_tags'])\n",
    "        f1.append(f1_score(real_tags, pred_tags))\n",
    "        \n",
    "        reports.append(classification_report(real_tags,pred_tags))\n",
    "\n",
    "        model_name = 'models/cnnlstm_f' + str(fold_no) + '.h5'\n",
    "\n",
    "        cnn_lstm.save(model_name)\n",
    "\n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "    return acc, loss, f1, reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnbilstm(inputs,targets,data_info):    \n",
    "    \n",
    "    acc = []\n",
    "    loss = []\n",
    "    f1 = []\n",
    "    reports = []\n",
    "\n",
    "    fold_no = 1\n",
    "\n",
    "    for train, test in kfold.split(inputs, targets):\n",
    "\n",
    "        #Modelo\n",
    "        cnn_bilstm = Sequential()\n",
    "        cnn_bilstm.add(Embedding(input_dim=data_info['words_amt']+1, output_dim=50, input_length=max_length))\n",
    "        cnn_bilstm.add(Conv1D(filters=data_info['tags_amt'], kernel_size=3, padding='same', activation='relu'))\n",
    "        cnn_bilstm.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "        cnn_bilstm.add(Dropout(0.5))\n",
    "        cnn_bilstm.add(Dense(data_info['tags_amt'], activation='sigmoid'))\n",
    "\n",
    "        adam = Adam(learning_rate=0.009)\n",
    "\n",
    "        cnn_bilstm.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='accuracy', min_delta=0, patience=5, verbose=0, mode='max', baseline=None, restore_best_weights=False)\n",
    "        callbacks = [early_stopping]\n",
    "\n",
    "        history = cnn_bilstm.fit(inputs[train],targets[train],batch_size=30, epochs=20, callbacks=callbacks)\n",
    "\n",
    "\n",
    "        scores = cnn_bilstm.evaluate(inputs[test], targets[test], verbose=0)\n",
    "        print(f'Score for fold {fold_no}: {cnn_bilstm.metrics_names[0]} of {scores[0]}; {cnn_bilstm.metrics_names[1]} of {scores[1]*100}%')\n",
    "        acc.append(scores[1] * 100)\n",
    "        loss.append(scores[0])\n",
    "\n",
    "\n",
    "        predictions = cnn_bilstm.predict(inputs[test], verbose=0)\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "        real_tags, pred_tags = convert_values(predictions,targets[test],data_info['i_tags'])\n",
    "        f1.append(f1_score(real_tags, pred_tags))\n",
    "        \n",
    "        reports.append(classification_report(real_tags,pred_tags))\n",
    "\n",
    "\n",
    "        model_name = 'models/cnnbilstm_f' + str(fold_no) + '.h5'\n",
    "\n",
    "\n",
    "        cnn_bilstm.save(model_name)\n",
    "\n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "    return acc, loss, f1, reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rodando o experimento para cada tipo de ato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, d = handle_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 400, 100)          2423800   \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 400, 100)          80400     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 400, 100)          0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 400, 35)           3535      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,507,735\n",
      "Trainable params: 83,935\n",
      "Non-trainable params: 2,423,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "116/116 [==============================] - 34s 265ms/step - loss: 0.4655 - accuracy: 0.8703\n",
      "Epoch 2/25\n",
      "116/116 [==============================] - 31s 270ms/step - loss: 0.1654 - accuracy: 0.9551\n",
      "Epoch 3/25\n",
      "116/116 [==============================] - 32s 277ms/step - loss: 0.1160 - accuracy: 0.9695\n",
      "Epoch 4/25\n",
      "116/116 [==============================] - 32s 272ms/step - loss: 0.0885 - accuracy: 0.9772\n",
      "Epoch 5/25\n",
      "116/116 [==============================] - 31s 266ms/step - loss: 0.0699 - accuracy: 0.9817\n",
      "Epoch 6/25\n",
      "116/116 [==============================] - 27s 231ms/step - loss: 0.0612 - accuracy: 0.9839\n",
      "Epoch 7/25\n",
      "116/116 [==============================] - 27s 235ms/step - loss: 0.0576 - accuracy: 0.9847\n",
      "Epoch 8/25\n",
      "116/116 [==============================] - 31s 265ms/step - loss: 0.0550 - accuracy: 0.9853\n",
      "Epoch 9/25\n",
      "116/116 [==============================] - 30s 263ms/step - loss: 0.0545 - accuracy: 0.9855\n",
      "Epoch 10/25\n",
      "116/116 [==============================] - 30s 262ms/step - loss: 0.0622 - accuracy: 0.9832\n",
      "Epoch 11/25\n",
      "116/116 [==============================] - 30s 261ms/step - loss: 0.0625 - accuracy: 0.9829\n",
      "Epoch 12/25\n",
      "116/116 [==============================] - 30s 257ms/step - loss: 0.0477 - accuracy: 0.9874\n",
      "Epoch 13/25\n",
      "116/116 [==============================] - 22s 186ms/step - loss: 0.0425 - accuracy: 0.9886\n",
      "Epoch 14/25\n",
      "116/116 [==============================] - 33s 290ms/step - loss: 0.0447 - accuracy: 0.9879\n",
      "Epoch 15/25\n",
      "116/116 [==============================] - 34s 294ms/step - loss: 0.0395 - accuracy: 0.9894\n",
      "Epoch 16/25\n",
      "116/116 [==============================] - 34s 294ms/step - loss: 0.0375 - accuracy: 0.9899\n",
      "Epoch 17/25\n",
      "116/116 [==============================] - 34s 292ms/step - loss: 0.0345 - accuracy: 0.9905\n",
      "Epoch 18/25\n",
      "116/116 [==============================] - 35s 303ms/step - loss: 0.0322 - accuracy: 0.9911\n",
      "Epoch 19/25\n",
      "116/116 [==============================] - 35s 300ms/step - loss: 0.0373 - accuracy: 0.9896\n",
      "Epoch 20/25\n",
      "116/116 [==============================] - 36s 306ms/step - loss: 0.0417 - accuracy: 0.9884\n",
      "Epoch 21/25\n",
      "116/116 [==============================] - 36s 308ms/step - loss: 0.0417 - accuracy: 0.9886\n",
      "Epoch 22/25\n",
      "116/116 [==============================] - 36s 307ms/step - loss: 0.0372 - accuracy: 0.9899\n",
      "Epoch 23/25\n",
      "116/116 [==============================] - 36s 313ms/step - loss: 0.0666 - accuracy: 0.9818\n",
      "Score for fold 1: loss of 0.1318674236536026; accuracy of 96.81124091148376%\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 400, 100)          2423800   \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 400, 100)          80400     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 400, 100)          0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 400, 35)           3535      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,507,735\n",
      "Trainable params: 83,935\n",
      "Non-trainable params: 2,423,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "116/116 [==============================] - 38s 300ms/step - loss: 0.4702 - accuracy: 0.8715\n",
      "Epoch 2/25\n",
      "116/116 [==============================] - 35s 302ms/step - loss: 0.1838 - accuracy: 0.9495\n",
      "Epoch 3/25\n",
      "116/116 [==============================] - 35s 304ms/step - loss: 0.1279 - accuracy: 0.9649\n",
      "Epoch 4/25\n",
      "116/116 [==============================] - 36s 307ms/step - loss: 0.0934 - accuracy: 0.9749\n",
      "Epoch 5/25\n",
      "116/116 [==============================] - 36s 308ms/step - loss: 0.0764 - accuracy: 0.9797\n",
      "Epoch 6/25\n",
      "116/116 [==============================] - 37s 317ms/step - loss: 0.0650 - accuracy: 0.9827\n",
      "Epoch 7/25\n",
      "116/116 [==============================] - 36s 307ms/step - loss: 0.0563 - accuracy: 0.9851\n",
      "Epoch 8/25\n",
      "116/116 [==============================] - 34s 298ms/step - loss: 0.0515 - accuracy: 0.9862\n",
      "Epoch 9/25\n",
      "116/116 [==============================] - 36s 311ms/step - loss: 0.0460 - accuracy: 0.9876\n",
      "Epoch 10/25\n",
      "116/116 [==============================] - 35s 306ms/step - loss: 0.0470 - accuracy: 0.9871\n",
      "Epoch 11/25\n",
      "116/116 [==============================] - 35s 300ms/step - loss: 0.0471 - accuracy: 0.9871\n",
      "Epoch 12/25\n",
      "116/116 [==============================] - 35s 301ms/step - loss: 0.0610 - accuracy: 0.9831\n",
      "Epoch 13/25\n",
      "116/116 [==============================] - 34s 295ms/step - loss: 0.0759 - accuracy: 0.9786\n",
      "Epoch 14/25\n",
      "116/116 [==============================] - 35s 300ms/step - loss: 0.0795 - accuracy: 0.9785\n",
      "Score for fold 2: loss of 0.10918459296226501; accuracy of 97.22550511360168%\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 400, 100)          2423800   \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 400, 100)          80400     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 400, 100)          0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 400, 35)           3535      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,507,735\n",
      "Trainable params: 83,935\n",
      "Non-trainable params: 2,423,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "116/116 [==============================] - 32s 243ms/step - loss: 0.4566 - accuracy: 0.8735\n",
      "Epoch 2/25\n",
      "116/116 [==============================] - 27s 236ms/step - loss: 0.1707 - accuracy: 0.9537\n",
      "Epoch 3/25\n",
      "116/116 [==============================] - 34s 292ms/step - loss: 0.1289 - accuracy: 0.9650\n",
      "Epoch 4/25\n",
      "116/116 [==============================] - 33s 288ms/step - loss: 0.0994 - accuracy: 0.9735\n",
      "Epoch 5/25\n",
      "116/116 [==============================] - 35s 303ms/step - loss: 0.0905 - accuracy: 0.9759\n",
      "Epoch 6/25\n",
      "116/116 [==============================] - 35s 302ms/step - loss: 0.0769 - accuracy: 0.9799\n",
      "Epoch 7/25\n",
      "116/116 [==============================] - 35s 304ms/step - loss: 0.0652 - accuracy: 0.9830\n",
      "Epoch 8/25\n",
      "116/116 [==============================] - 34s 296ms/step - loss: 0.0574 - accuracy: 0.9849\n",
      "Epoch 9/25\n",
      "116/116 [==============================] - 33s 285ms/step - loss: 0.0693 - accuracy: 0.9819\n",
      "Epoch 10/25\n",
      "116/116 [==============================] - 36s 306ms/step - loss: 0.0631 - accuracy: 0.9832\n",
      "Epoch 11/25\n",
      "116/116 [==============================] - 34s 297ms/step - loss: 0.0625 - accuracy: 0.9834\n",
      "Epoch 12/25\n",
      "116/116 [==============================] - 35s 300ms/step - loss: 0.0576 - accuracy: 0.9852\n",
      "Epoch 13/25\n",
      "116/116 [==============================] - 21s 177ms/step - loss: 0.1365 - accuracy: 0.9628\n",
      "Epoch 14/25\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 0.1433 - accuracy: 0.9607\n",
      "Epoch 15/25\n",
      "116/116 [==============================] - 18s 152ms/step - loss: 0.0963 - accuracy: 0.9742\n",
      "Epoch 16/25\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 0.0784 - accuracy: 0.9792\n",
      "Epoch 17/25\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 0.0740 - accuracy: 0.9799\n",
      "Score for fold 3: loss of 0.09034620970487595; accuracy of 97.68011569976807%\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 400, 100)          2423800   \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 400, 100)          80400     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 400, 100)          0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 400, 35)           3535      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,507,735\n",
      "Trainable params: 83,935\n",
      "Non-trainable params: 2,423,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "116/116 [==============================] - 22s 174ms/step - loss: 0.4556 - accuracy: 0.8774\n",
      "Epoch 2/25\n",
      "116/116 [==============================] - 20s 175ms/step - loss: 0.1810 - accuracy: 0.9509\n",
      "Epoch 3/25\n",
      "116/116 [==============================] - 20s 173ms/step - loss: 0.1222 - accuracy: 0.9666\n",
      "Epoch 4/25\n",
      "116/116 [==============================] - 17s 149ms/step - loss: 0.1050 - accuracy: 0.9712\n",
      "Epoch 5/25\n",
      "116/116 [==============================] - 17s 148ms/step - loss: 0.0868 - accuracy: 0.9767\n",
      "Epoch 6/25\n",
      "116/116 [==============================] - 17s 148ms/step - loss: 0.0748 - accuracy: 0.9797\n",
      "Epoch 7/25\n",
      "116/116 [==============================] - 19s 164ms/step - loss: 0.0722 - accuracy: 0.9806\n",
      "Epoch 8/25\n",
      "116/116 [==============================] - 17s 143ms/step - loss: 0.0601 - accuracy: 0.9841\n",
      "Epoch 9/25\n",
      "116/116 [==============================] - 16s 142ms/step - loss: 0.0544 - accuracy: 0.9856\n",
      "Epoch 10/25\n",
      "116/116 [==============================] - 18s 154ms/step - loss: 0.0493 - accuracy: 0.9870\n",
      "Epoch 11/25\n",
      "116/116 [==============================] - 18s 154ms/step - loss: 0.0458 - accuracy: 0.9877\n",
      "Epoch 12/25\n",
      "116/116 [==============================] - 23s 201ms/step - loss: 0.0446 - accuracy: 0.9878\n",
      "Epoch 13/25\n",
      "116/116 [==============================] - 25s 217ms/step - loss: 0.0505 - accuracy: 0.9863\n",
      "Epoch 14/25\n",
      "116/116 [==============================] - 25s 219ms/step - loss: 0.0450 - accuracy: 0.9877\n",
      "Epoch 15/25\n",
      "116/116 [==============================] - 25s 215ms/step - loss: 0.0401 - accuracy: 0.9890\n",
      "Epoch 16/25\n",
      "116/116 [==============================] - 27s 230ms/step - loss: 0.0666 - accuracy: 0.9818\n",
      "Epoch 17/25\n",
      "116/116 [==============================] - 25s 211ms/step - loss: 0.1132 - accuracy: 0.9685\n",
      "Epoch 18/25\n",
      "116/116 [==============================] - 25s 211ms/step - loss: 0.0912 - accuracy: 0.9748\n",
      "Epoch 19/25\n",
      "116/116 [==============================] - 25s 215ms/step - loss: 0.0900 - accuracy: 0.9755\n",
      "Epoch 20/25\n",
      "116/116 [==============================] - 26s 223ms/step - loss: 0.0701 - accuracy: 0.9810\n",
      "Score for fold 4: loss of 0.12376803904771805; accuracy of 97.03890681266785%\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 400, 100)          2423800   \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 400, 100)          80400     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 400, 100)          0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 400, 35)           3535      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,507,735\n",
      "Trainable params: 83,935\n",
      "Non-trainable params: 2,423,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "116/116 [==============================] - 28s 219ms/step - loss: 0.4627 - accuracy: 0.8755\n",
      "Epoch 2/25\n",
      "116/116 [==============================] - 27s 229ms/step - loss: 0.1689 - accuracy: 0.9541\n",
      "Epoch 3/25\n",
      "116/116 [==============================] - 23s 201ms/step - loss: 0.1125 - accuracy: 0.9701\n",
      "Epoch 4/25\n",
      "116/116 [==============================] - 27s 229ms/step - loss: 0.0845 - accuracy: 0.9777\n",
      "Epoch 5/25\n",
      "116/116 [==============================] - 25s 216ms/step - loss: 0.0772 - accuracy: 0.9798\n",
      "Epoch 6/25\n",
      "116/116 [==============================] - 25s 211ms/step - loss: 0.0752 - accuracy: 0.9799\n",
      "Epoch 7/25\n",
      "116/116 [==============================] - 25s 216ms/step - loss: 0.0781 - accuracy: 0.9791\n",
      "Epoch 8/25\n",
      "116/116 [==============================] - 24s 211ms/step - loss: 0.0722 - accuracy: 0.9805\n",
      "Epoch 9/25\n",
      "116/116 [==============================] - 19s 166ms/step - loss: 0.0571 - accuracy: 0.9849\n",
      "Epoch 10/25\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 0.0513 - accuracy: 0.9865\n",
      "Epoch 11/25\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 0.0555 - accuracy: 0.9852\n",
      "Epoch 12/25\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 0.0471 - accuracy: 0.9876\n",
      "Epoch 13/25\n",
      "116/116 [==============================] - 18s 152ms/step - loss: 0.0437 - accuracy: 0.9884\n",
      "Epoch 14/25\n",
      "116/116 [==============================] - 18s 153ms/step - loss: 0.0402 - accuracy: 0.9892\n",
      "Epoch 15/25\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 0.0368 - accuracy: 0.9900\n",
      "Epoch 16/25\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 0.0355 - accuracy: 0.9904\n",
      "Epoch 17/25\n",
      "116/116 [==============================] - 17s 150ms/step - loss: 0.0349 - accuracy: 0.9905\n",
      "Epoch 18/25\n",
      "116/116 [==============================] - 17s 147ms/step - loss: 0.0432 - accuracy: 0.9880\n",
      "Epoch 19/25\n",
      "116/116 [==============================] - 19s 161ms/step - loss: 0.0573 - accuracy: 0.9841\n",
      "Epoch 20/25\n",
      "116/116 [==============================] - 18s 157ms/step - loss: 0.0868 - accuracy: 0.9757\n",
      "Epoch 21/25\n",
      "116/116 [==============================] - 20s 173ms/step - loss: 0.0831 - accuracy: 0.9778\n",
      "Epoch 22/25\n",
      "116/116 [==============================] - 17s 145ms/step - loss: 0.0608 - accuracy: 0.9838\n",
      "Score for fold 5: loss of 0.09996425360441208; accuracy of 97.73916006088257%\n"
     ]
    }
   ],
   "source": [
    "lstm_acc, lstm_loss, lstm_f1, lstm_reports = lstm(x,y,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7355920363255328,\n",
       " 0.7483482091109308,\n",
       " 0.7875824425744826,\n",
       " 0.7920129270544782,\n",
       " 0.8211873444720938]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7769445919075035"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lstm_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031065822724061762"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(lstm_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "62/62 [==============================] - 18s 259ms/step - loss: 1.4576 - accuracy: 0.7772\n",
      "Epoch 2/20\n",
      "62/62 [==============================] - 15s 247ms/step - loss: 0.7115 - accuracy: 0.8175\n",
      "Epoch 3/20\n",
      "62/62 [==============================] - 15s 240ms/step - loss: 0.5075 - accuracy: 0.8579\n",
      "Epoch 4/20\n",
      "62/62 [==============================] - 15s 244ms/step - loss: 0.4181 - accuracy: 0.8727\n",
      "Epoch 5/20\n",
      "62/62 [==============================] - 15s 245ms/step - loss: 0.3425 - accuracy: 0.9002\n",
      "Epoch 6/20\n",
      "62/62 [==============================] - 14s 225ms/step - loss: 0.2591 - accuracy: 0.9352\n",
      "Epoch 7/20\n",
      "62/62 [==============================] - 14s 228ms/step - loss: 0.2035 - accuracy: 0.9497\n",
      "Epoch 8/20\n",
      "62/62 [==============================] - 14s 232ms/step - loss: 0.1721 - accuracy: 0.9569\n",
      "Epoch 9/20\n",
      "62/62 [==============================] - 16s 250ms/step - loss: 0.1527 - accuracy: 0.9615\n",
      "Epoch 10/20\n",
      "62/62 [==============================] - 15s 244ms/step - loss: 0.1401 - accuracy: 0.9644\n",
      "Epoch 11/20\n",
      "62/62 [==============================] - 15s 247ms/step - loss: 0.1436 - accuracy: 0.9626\n",
      "Epoch 12/20\n",
      "62/62 [==============================] - 15s 244ms/step - loss: 0.1273 - accuracy: 0.9673\n",
      "Epoch 13/20\n",
      "62/62 [==============================] - 14s 234ms/step - loss: 0.1174 - accuracy: 0.9701\n",
      "Epoch 14/20\n",
      "62/62 [==============================] - 14s 233ms/step - loss: 0.1133 - accuracy: 0.9719\n",
      "Epoch 15/20\n",
      "62/62 [==============================] - 16s 252ms/step - loss: 0.0968 - accuracy: 0.9759\n",
      "Epoch 16/20\n",
      "62/62 [==============================] - 15s 240ms/step - loss: 0.0873 - accuracy: 0.9787\n",
      "Epoch 17/20\n",
      "62/62 [==============================] - 15s 244ms/step - loss: 0.0835 - accuracy: 0.9796\n",
      "Epoch 18/20\n",
      "62/62 [==============================] - 15s 245ms/step - loss: 0.0818 - accuracy: 0.9798\n",
      "Epoch 19/20\n",
      "62/62 [==============================] - 16s 258ms/step - loss: 0.1045 - accuracy: 0.9746\n",
      "Epoch 20/20\n",
      "62/62 [==============================] - 16s 265ms/step - loss: 0.0730 - accuracy: 0.9828\n",
      "Score for fold 1: loss of 0.22698235511779785; accuracy of 95.44146060943604%\n",
      "Epoch 1/20\n",
      "62/62 [==============================] - 18s 264ms/step - loss: 1.4773 - accuracy: 0.7738\n",
      "Epoch 2/20\n",
      "62/62 [==============================] - 18s 286ms/step - loss: 0.6883 - accuracy: 0.8214\n",
      "Epoch 3/20\n",
      "62/62 [==============================] - 18s 288ms/step - loss: 0.5113 - accuracy: 0.8561\n",
      "Epoch 4/20\n",
      "62/62 [==============================] - 17s 281ms/step - loss: 0.4367 - accuracy: 0.8690\n",
      "Epoch 5/20\n",
      "62/62 [==============================] - 17s 275ms/step - loss: 0.3490 - accuracy: 0.9041\n",
      "Epoch 6/20\n",
      "62/62 [==============================] - 18s 294ms/step - loss: 0.2702 - accuracy: 0.9321\n",
      "Epoch 7/20\n",
      "62/62 [==============================] - 19s 310ms/step - loss: 0.2666 - accuracy: 0.9332\n",
      "Epoch 8/20\n",
      "62/62 [==============================] - 19s 310ms/step - loss: 0.1977 - accuracy: 0.9513\n",
      "Epoch 9/20\n",
      "62/62 [==============================] - 18s 284ms/step - loss: 0.1729 - accuracy: 0.9568\n",
      "Epoch 10/20\n",
      "62/62 [==============================] - 19s 302ms/step - loss: 0.1581 - accuracy: 0.9600\n",
      "Epoch 11/20\n",
      "62/62 [==============================] - 15s 235ms/step - loss: 0.1497 - accuracy: 0.9621\n",
      "Epoch 12/20\n",
      "62/62 [==============================] - 18s 283ms/step - loss: 0.1310 - accuracy: 0.9671\n",
      "Epoch 13/20\n",
      "62/62 [==============================] - 19s 310ms/step - loss: 0.1188 - accuracy: 0.9705\n",
      "Epoch 14/20\n",
      "62/62 [==============================] - 15s 244ms/step - loss: 0.1074 - accuracy: 0.9734\n",
      "Epoch 15/20\n",
      "62/62 [==============================] - 15s 237ms/step - loss: 0.1049 - accuracy: 0.9742\n",
      "Epoch 16/20\n",
      "62/62 [==============================] - 15s 240ms/step - loss: 0.0983 - accuracy: 0.9762\n",
      "Epoch 17/20\n",
      "62/62 [==============================] - 15s 246ms/step - loss: 0.1346 - accuracy: 0.9658\n",
      "Epoch 18/20\n",
      "62/62 [==============================] - 15s 244ms/step - loss: 0.0900 - accuracy: 0.9782\n",
      "Epoch 19/20\n",
      "62/62 [==============================] - 15s 249ms/step - loss: 0.0801 - accuracy: 0.9808\n",
      "Epoch 20/20\n",
      "62/62 [==============================] - 15s 248ms/step - loss: 0.0737 - accuracy: 0.9825\n",
      "Score for fold 2: loss of 0.21112366020679474; accuracy of 95.62520384788513%\n",
      "Epoch 1/20\n",
      "62/62 [==============================] - 17s 243ms/step - loss: 1.4794 - accuracy: 0.7725\n",
      "Epoch 2/20\n",
      "62/62 [==============================] - 16s 250ms/step - loss: 0.6818 - accuracy: 0.8251\n",
      "Epoch 3/20\n",
      "62/62 [==============================] - 15s 250ms/step - loss: 0.4858 - accuracy: 0.8620\n",
      "Epoch 4/20\n",
      "62/62 [==============================] - 15s 249ms/step - loss: 0.3687 - accuracy: 0.9002\n",
      "Epoch 5/20\n",
      "62/62 [==============================] - 16s 251ms/step - loss: 0.3084 - accuracy: 0.9219\n",
      "Epoch 6/20\n",
      "62/62 [==============================] - 15s 245ms/step - loss: 0.2354 - accuracy: 0.9436\n",
      "Epoch 7/20\n",
      "62/62 [==============================] - 15s 247ms/step - loss: 0.1988 - accuracy: 0.9517\n",
      "Epoch 8/20\n",
      "62/62 [==============================] - 15s 250ms/step - loss: 0.1763 - accuracy: 0.9559\n",
      "Epoch 9/20\n",
      "62/62 [==============================] - 15s 248ms/step - loss: 0.1574 - accuracy: 0.9598\n",
      "Epoch 10/20\n",
      "62/62 [==============================] - 14s 230ms/step - loss: 0.1427 - accuracy: 0.9636\n",
      "Epoch 11/20\n",
      "62/62 [==============================] - 14s 232ms/step - loss: 0.1283 - accuracy: 0.9670\n",
      "Epoch 12/20\n",
      "62/62 [==============================] - 14s 232ms/step - loss: 0.1226 - accuracy: 0.9684\n",
      "Epoch 13/20\n",
      "62/62 [==============================] - 15s 237ms/step - loss: 0.1380 - accuracy: 0.9639\n",
      "Epoch 14/20\n",
      "62/62 [==============================] - 16s 265ms/step - loss: 0.1332 - accuracy: 0.9656\n",
      "Epoch 15/20\n",
      "62/62 [==============================] - 16s 257ms/step - loss: 0.1159 - accuracy: 0.9707\n",
      "Epoch 16/20\n",
      "62/62 [==============================] - 16s 253ms/step - loss: 0.0934 - accuracy: 0.9768\n",
      "Epoch 17/20\n",
      "62/62 [==============================] - 17s 267ms/step - loss: 0.0852 - accuracy: 0.9788\n",
      "Epoch 18/20\n",
      "62/62 [==============================] - 18s 285ms/step - loss: 0.0796 - accuracy: 0.9804\n",
      "Epoch 19/20\n",
      "62/62 [==============================] - 18s 283ms/step - loss: 0.0747 - accuracy: 0.9817\n",
      "Epoch 20/20\n",
      "62/62 [==============================] - 18s 284ms/step - loss: 0.0715 - accuracy: 0.9830\n",
      "Score for fold 3: loss of 0.2217930257320404; accuracy of 95.81219553947449%\n",
      "Epoch 1/20\n",
      "62/62 [==============================] - 17s 248ms/step - loss: 1.4372 - accuracy: 0.7767\n",
      "Epoch 2/20\n",
      "62/62 [==============================] - 16s 261ms/step - loss: 0.7384 - accuracy: 0.8150\n",
      "Epoch 3/20\n",
      "62/62 [==============================] - 16s 257ms/step - loss: 0.5168 - accuracy: 0.8564\n",
      "Epoch 4/20\n",
      "62/62 [==============================] - 16s 252ms/step - loss: 0.4174 - accuracy: 0.8750\n",
      "Epoch 5/20\n",
      "62/62 [==============================] - 16s 260ms/step - loss: 0.3146 - accuracy: 0.9187\n",
      "Epoch 6/20\n",
      "62/62 [==============================] - 17s 281ms/step - loss: 0.2396 - accuracy: 0.9418\n",
      "Epoch 7/20\n",
      "62/62 [==============================] - 17s 281ms/step - loss: 0.1891 - accuracy: 0.9540\n",
      "Epoch 8/20\n",
      "62/62 [==============================] - 16s 264ms/step - loss: 0.1765 - accuracy: 0.9558\n",
      "Epoch 9/20\n",
      "62/62 [==============================] - 18s 285ms/step - loss: 0.1476 - accuracy: 0.9632\n",
      "Epoch 10/20\n",
      "62/62 [==============================] - 18s 287ms/step - loss: 0.1327 - accuracy: 0.9668\n",
      "Epoch 11/20\n",
      "62/62 [==============================] - 18s 287ms/step - loss: 0.1296 - accuracy: 0.9667\n",
      "Epoch 12/20\n",
      "62/62 [==============================] - 16s 260ms/step - loss: 0.1104 - accuracy: 0.9724\n",
      "Epoch 13/20\n",
      "62/62 [==============================] - 14s 230ms/step - loss: 0.0990 - accuracy: 0.9755\n",
      "Epoch 14/20\n",
      "62/62 [==============================] - 15s 234ms/step - loss: 0.0930 - accuracy: 0.9770\n",
      "Epoch 15/20\n",
      "62/62 [==============================] - 15s 244ms/step - loss: 0.0847 - accuracy: 0.9793\n",
      "Epoch 16/20\n",
      "62/62 [==============================] - 15s 247ms/step - loss: 0.0776 - accuracy: 0.9813\n",
      "Epoch 17/20\n",
      "62/62 [==============================] - 16s 250ms/step - loss: 0.0741 - accuracy: 0.9822\n",
      "Epoch 18/20\n",
      "62/62 [==============================] - 16s 263ms/step - loss: 0.0676 - accuracy: 0.9839\n",
      "Epoch 19/20\n",
      "62/62 [==============================] - 17s 268ms/step - loss: 0.0694 - accuracy: 0.9834\n",
      "Epoch 20/20\n",
      "62/62 [==============================] - 17s 270ms/step - loss: 0.0798 - accuracy: 0.9812\n",
      "Score for fold 4: loss of 0.23286832869052887; accuracy of 95.32439112663269%\n",
      "Epoch 1/20\n",
      "62/62 [==============================] - 20s 287ms/step - loss: 1.4277 - accuracy: 0.7826\n",
      "Epoch 2/20\n",
      "62/62 [==============================] - 18s 295ms/step - loss: 0.7144 - accuracy: 0.8140\n",
      "Epoch 3/20\n",
      "62/62 [==============================] - 15s 241ms/step - loss: 0.5069 - accuracy: 0.8555\n",
      "Epoch 4/20\n",
      "62/62 [==============================] - 15s 239ms/step - loss: 0.3935 - accuracy: 0.8919\n",
      "Epoch 5/20\n",
      "62/62 [==============================] - 16s 264ms/step - loss: 0.2887 - accuracy: 0.9312\n",
      "Epoch 6/20\n",
      "62/62 [==============================] - 17s 269ms/step - loss: 0.2575 - accuracy: 0.9362\n",
      "Epoch 7/20\n",
      "62/62 [==============================] - 16s 263ms/step - loss: 0.2453 - accuracy: 0.9380\n",
      "Epoch 8/20\n",
      "62/62 [==============================] - 16s 259ms/step - loss: 0.2101 - accuracy: 0.9485\n",
      "Epoch 9/20\n",
      "62/62 [==============================] - 16s 259ms/step - loss: 0.1738 - accuracy: 0.9567\n",
      "Epoch 10/20\n",
      "62/62 [==============================] - 17s 278ms/step - loss: 0.1535 - accuracy: 0.9617\n",
      "Epoch 11/20\n",
      "62/62 [==============================] - 16s 250ms/step - loss: 0.1380 - accuracy: 0.9650\n",
      "Epoch 12/20\n",
      "62/62 [==============================] - 16s 260ms/step - loss: 0.1258 - accuracy: 0.9679\n",
      "Epoch 13/20\n",
      "62/62 [==============================] - 17s 270ms/step - loss: 0.1152 - accuracy: 0.9706\n",
      "Epoch 14/20\n",
      "62/62 [==============================] - 18s 291ms/step - loss: 0.1064 - accuracy: 0.9731\n",
      "Epoch 15/20\n",
      "62/62 [==============================] - 17s 279ms/step - loss: 0.0972 - accuracy: 0.9755\n",
      "Epoch 16/20\n",
      "62/62 [==============================] - 17s 278ms/step - loss: 0.0902 - accuracy: 0.9773\n",
      "Epoch 17/20\n",
      "62/62 [==============================] - 17s 280ms/step - loss: 0.0849 - accuracy: 0.9790\n",
      "Epoch 18/20\n",
      "62/62 [==============================] - 17s 281ms/step - loss: 0.0783 - accuracy: 0.9809\n",
      "Epoch 19/20\n",
      "62/62 [==============================] - 18s 284ms/step - loss: 0.0721 - accuracy: 0.9826\n",
      "Epoch 20/20\n",
      "62/62 [==============================] - 17s 278ms/step - loss: 0.0689 - accuracy: 0.9837\n",
      "Score for fold 5: loss of 0.21266479790210724; accuracy of 95.77280282974243%\n"
     ]
    }
   ],
   "source": [
    "cnnlstm_acc, cnnlstm_loss, cnnlstm_f1, cnnlstm_reports = cnnlstm(x,y,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnnlstm_f1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[39m.\u001b[39mmean(cnnlstm_f1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cnnlstm_f1' is not defined"
     ]
    }
   ],
   "source": [
    "np.mean(cnnlstm_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01345543566426264"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(cnnlstm_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5937359777438751,\n",
       " 0.566768039811999,\n",
       " 0.5619954648526078,\n",
       " 0.5679884120948759,\n",
       " 0.5916870415647922]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnnlstm_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "82/82 [==============================] - 28s 295ms/step - loss: 1.1901 - accuracy: 0.7890\n",
      "Epoch 2/20\n",
      "82/82 [==============================] - 23s 279ms/step - loss: 0.5082 - accuracy: 0.8584\n",
      "Epoch 3/20\n",
      "82/82 [==============================] - 23s 284ms/step - loss: 0.3379 - accuracy: 0.9166\n",
      "Epoch 4/20\n",
      "82/82 [==============================] - 24s 293ms/step - loss: 0.2308 - accuracy: 0.9424\n",
      "Epoch 5/20\n",
      "82/82 [==============================] - 24s 296ms/step - loss: 0.1837 - accuracy: 0.9509\n",
      "Epoch 6/20\n",
      "82/82 [==============================] - 23s 285ms/step - loss: 0.1493 - accuracy: 0.9611\n",
      "Epoch 7/20\n",
      "82/82 [==============================] - 23s 285ms/step - loss: 0.1171 - accuracy: 0.9700\n",
      "Epoch 8/20\n",
      "82/82 [==============================] - 24s 290ms/step - loss: 0.0978 - accuracy: 0.9755\n",
      "Epoch 9/20\n",
      "82/82 [==============================] - 24s 291ms/step - loss: 0.0867 - accuracy: 0.9783\n",
      "Epoch 10/20\n",
      "82/82 [==============================] - 24s 289ms/step - loss: 0.0760 - accuracy: 0.9808\n",
      "Epoch 11/20\n",
      "82/82 [==============================] - 24s 289ms/step - loss: 0.0632 - accuracy: 0.9844\n",
      "Epoch 12/20\n",
      "82/82 [==============================] - 24s 291ms/step - loss: 0.0562 - accuracy: 0.9864\n",
      "Epoch 13/20\n",
      "82/82 [==============================] - 24s 290ms/step - loss: 0.0549 - accuracy: 0.9862\n",
      "Epoch 14/20\n",
      "82/82 [==============================] - 24s 287ms/step - loss: 0.0475 - accuracy: 0.9882\n",
      "Epoch 15/20\n",
      "82/82 [==============================] - 24s 287ms/step - loss: 0.0427 - accuracy: 0.9893\n",
      "Epoch 16/20\n",
      "82/82 [==============================] - 24s 289ms/step - loss: 0.0373 - accuracy: 0.9908\n",
      "Epoch 17/20\n",
      "82/82 [==============================] - 24s 289ms/step - loss: 0.0437 - accuracy: 0.9886\n",
      "Epoch 18/20\n",
      "82/82 [==============================] - 23s 286ms/step - loss: 0.0386 - accuracy: 0.9903\n",
      "Epoch 19/20\n",
      "82/82 [==============================] - 24s 290ms/step - loss: 0.0321 - accuracy: 0.9920\n",
      "Epoch 20/20\n",
      "82/82 [==============================] - 24s 290ms/step - loss: 0.0285 - accuracy: 0.9929\n",
      "Score for fold 1: loss of 0.20901209115982056; accuracy of 95.92927098274231%\n",
      "Epoch 1/20\n",
      "82/82 [==============================] - 26s 286ms/step - loss: 1.2258 - accuracy: 0.7867\n",
      "Epoch 2/20\n",
      "82/82 [==============================] - 24s 290ms/step - loss: 0.5283 - accuracy: 0.8518\n",
      "Epoch 3/20\n",
      "82/82 [==============================] - 24s 292ms/step - loss: 0.3553 - accuracy: 0.9075\n",
      "Epoch 4/20\n",
      "82/82 [==============================] - 24s 291ms/step - loss: 0.2253 - accuracy: 0.9416\n",
      "Epoch 5/20\n",
      "82/82 [==============================] - 24s 292ms/step - loss: 0.1655 - accuracy: 0.9564\n",
      "Epoch 6/20\n",
      "82/82 [==============================] - 24s 288ms/step - loss: 0.1316 - accuracy: 0.9648\n",
      "Epoch 7/20\n",
      "82/82 [==============================] - 24s 289ms/step - loss: 0.1092 - accuracy: 0.9716\n",
      "Epoch 8/20\n",
      "82/82 [==============================] - 24s 290ms/step - loss: 0.0895 - accuracy: 0.9769\n",
      "Epoch 9/20\n",
      "82/82 [==============================] - 24s 297ms/step - loss: 0.0751 - accuracy: 0.9808\n",
      "Epoch 10/20\n",
      "82/82 [==============================] - 24s 297ms/step - loss: 0.0637 - accuracy: 0.9840\n",
      "Epoch 11/20\n",
      "82/82 [==============================] - 24s 296ms/step - loss: 0.0579 - accuracy: 0.9856\n",
      "Epoch 12/20\n",
      "82/82 [==============================] - 24s 297ms/step - loss: 0.0495 - accuracy: 0.9877\n",
      "Epoch 13/20\n",
      "82/82 [==============================] - 24s 296ms/step - loss: 0.0440 - accuracy: 0.9891\n",
      "Epoch 14/20\n",
      "82/82 [==============================] - 24s 297ms/step - loss: 0.0473 - accuracy: 0.9877\n",
      "Epoch 15/20\n",
      "82/82 [==============================] - 24s 296ms/step - loss: 0.0508 - accuracy: 0.9869\n",
      "Epoch 16/20\n",
      "82/82 [==============================] - 24s 296ms/step - loss: 0.0368 - accuracy: 0.9908\n",
      "Epoch 17/20\n",
      "82/82 [==============================] - 24s 292ms/step - loss: 0.0359 - accuracy: 0.9908\n",
      "Epoch 18/20\n",
      "82/82 [==============================] - 24s 293ms/step - loss: 0.0305 - accuracy: 0.9923\n",
      "Epoch 19/20\n",
      "82/82 [==============================] - 24s 296ms/step - loss: 0.0332 - accuracy: 0.9914\n",
      "Epoch 20/20\n",
      "82/82 [==============================] - 24s 291ms/step - loss: 0.0283 - accuracy: 0.9928\n",
      "Score for fold 2: loss of 0.13833437860012054; accuracy of 97.00610041618347%\n",
      "Epoch 1/20\n",
      "82/82 [==============================] - 26s 294ms/step - loss: 1.1882 - accuracy: 0.7857\n",
      "Epoch 2/20\n",
      "82/82 [==============================] - 24s 295ms/step - loss: 0.5165 - accuracy: 0.8546\n",
      "Epoch 3/20\n",
      "82/82 [==============================] - 24s 295ms/step - loss: 0.3332 - accuracy: 0.9171\n",
      "Epoch 4/20\n",
      "82/82 [==============================] - 24s 296ms/step - loss: 0.2293 - accuracy: 0.9422\n",
      "Epoch 5/20\n",
      "82/82 [==============================] - 24s 295ms/step - loss: 0.1680 - accuracy: 0.9561\n",
      "Epoch 6/20\n",
      "82/82 [==============================] - 24s 290ms/step - loss: 0.1271 - accuracy: 0.9671\n",
      "Epoch 7/20\n",
      "82/82 [==============================] - 23s 286ms/step - loss: 0.1288 - accuracy: 0.9648\n",
      "Epoch 8/20\n",
      "82/82 [==============================] - 24s 295ms/step - loss: 0.1082 - accuracy: 0.9719\n",
      "Epoch 9/20\n",
      "82/82 [==============================] - 24s 292ms/step - loss: 0.0960 - accuracy: 0.9751\n",
      "Epoch 10/20\n",
      "82/82 [==============================] - 24s 288ms/step - loss: 0.0703 - accuracy: 0.9827\n",
      "Epoch 11/20\n",
      "82/82 [==============================] - 24s 292ms/step - loss: 0.0621 - accuracy: 0.9846\n",
      "Epoch 12/20\n",
      "82/82 [==============================] - 24s 289ms/step - loss: 0.0531 - accuracy: 0.9869\n",
      "Epoch 13/20\n",
      "82/82 [==============================] - 24s 295ms/step - loss: 0.0479 - accuracy: 0.9882\n",
      "Epoch 14/20\n",
      "82/82 [==============================] - 24s 292ms/step - loss: 0.0542 - accuracy: 0.9861\n",
      "Epoch 15/20\n",
      "82/82 [==============================] - 24s 288ms/step - loss: 0.0582 - accuracy: 0.9845\n",
      "Epoch 16/20\n",
      "82/82 [==============================] - 24s 291ms/step - loss: 0.0415 - accuracy: 0.9896\n",
      "Epoch 17/20\n",
      "82/82 [==============================] - 24s 293ms/step - loss: 0.0356 - accuracy: 0.9913\n",
      "Epoch 18/20\n",
      "82/82 [==============================] - 24s 294ms/step - loss: 0.0323 - accuracy: 0.9920\n",
      "Epoch 19/20\n",
      "82/82 [==============================] - 24s 294ms/step - loss: 0.0298 - accuracy: 0.9925\n",
      "Epoch 20/20\n",
      "82/82 [==============================] - 24s 295ms/step - loss: 0.0288 - accuracy: 0.9929\n",
      "Score for fold 3: loss of 0.19301056861877441; accuracy of 96.32073044776917%\n",
      "Epoch 1/20\n",
      "82/82 [==============================] - 27s 294ms/step - loss: 1.1881 - accuracy: 0.7907\n",
      "Epoch 2/20\n",
      "82/82 [==============================] - 24s 294ms/step - loss: 0.5278 - accuracy: 0.8489\n",
      "Epoch 3/20\n",
      "82/82 [==============================] - 24s 294ms/step - loss: 0.3917 - accuracy: 0.8845\n",
      "Epoch 4/20\n",
      "82/82 [==============================] - 24s 297ms/step - loss: 0.2548 - accuracy: 0.9301\n",
      "Epoch 5/20\n",
      "82/82 [==============================] - 24s 294ms/step - loss: 0.1862 - accuracy: 0.9493\n",
      "Epoch 6/20\n",
      "82/82 [==============================] - 24s 296ms/step - loss: 0.1442 - accuracy: 0.9615\n",
      "Epoch 7/20\n",
      "82/82 [==============================] - 24s 296ms/step - loss: 0.1112 - accuracy: 0.9710\n",
      "Epoch 8/20\n",
      "82/82 [==============================] - 24s 291ms/step - loss: 0.0971 - accuracy: 0.9745\n",
      "Epoch 9/20\n",
      "82/82 [==============================] - 24s 295ms/step - loss: 0.0886 - accuracy: 0.9766\n",
      "Epoch 10/20\n",
      "82/82 [==============================] - 23s 283ms/step - loss: 0.1469 - accuracy: 0.9567\n",
      "Epoch 11/20\n",
      "82/82 [==============================] - 23s 284ms/step - loss: 0.0793 - accuracy: 0.9793\n",
      "Epoch 12/20\n",
      "82/82 [==============================] - 24s 289ms/step - loss: 0.0666 - accuracy: 0.9828\n",
      "Epoch 13/20\n",
      "82/82 [==============================] - 23s 286ms/step - loss: 0.0577 - accuracy: 0.9853\n",
      "Epoch 14/20\n",
      "82/82 [==============================] - 24s 295ms/step - loss: 0.0513 - accuracy: 0.9869\n",
      "Epoch 15/20\n",
      "82/82 [==============================] - 24s 293ms/step - loss: 0.0458 - accuracy: 0.9884\n",
      "Epoch 16/20\n",
      "82/82 [==============================] - 24s 294ms/step - loss: 0.0488 - accuracy: 0.9873\n",
      "Epoch 17/20\n",
      "82/82 [==============================] - 24s 296ms/step - loss: 0.0410 - accuracy: 0.9895\n",
      "Epoch 18/20\n",
      "82/82 [==============================] - 24s 294ms/step - loss: 0.0377 - accuracy: 0.9904\n",
      "Epoch 19/20\n",
      "82/82 [==============================] - 24s 294ms/step - loss: 0.0348 - accuracy: 0.9911\n",
      "Epoch 20/20\n",
      "82/82 [==============================] - 23s 286ms/step - loss: 0.0344 - accuracy: 0.9911\n",
      "Score for fold 4: loss of 0.19484476745128632; accuracy of 95.93861699104309%\n",
      "Epoch 1/20\n",
      "82/82 [==============================] - 29s 309ms/step - loss: 1.2050 - accuracy: 0.7822\n",
      "Epoch 2/20\n",
      "82/82 [==============================] - 26s 316ms/step - loss: 0.5303 - accuracy: 0.8496\n",
      "Epoch 3/20\n",
      "82/82 [==============================] - 24s 297ms/step - loss: 0.3823 - accuracy: 0.8972\n",
      "Epoch 4/20\n",
      "82/82 [==============================] - 24s 295ms/step - loss: 0.2663 - accuracy: 0.9323\n",
      "Epoch 5/20\n",
      "82/82 [==============================] - 23s 278ms/step - loss: 0.2056 - accuracy: 0.9463\n",
      "Epoch 6/20\n",
      "82/82 [==============================] - 23s 281ms/step - loss: 0.1659 - accuracy: 0.9556\n",
      "Epoch 7/20\n",
      "82/82 [==============================] - 21s 255ms/step - loss: 0.1264 - accuracy: 0.9666\n",
      "Epoch 8/20\n",
      "82/82 [==============================] - 21s 262ms/step - loss: 0.1078 - accuracy: 0.9720\n",
      "Epoch 9/20\n",
      "82/82 [==============================] - 22s 267ms/step - loss: 0.0983 - accuracy: 0.9743\n",
      "Epoch 10/20\n",
      "82/82 [==============================] - 20s 247ms/step - loss: 0.0791 - accuracy: 0.9800\n",
      "Epoch 11/20\n",
      "82/82 [==============================] - 21s 252ms/step - loss: 0.0687 - accuracy: 0.9827\n",
      "Epoch 12/20\n",
      "82/82 [==============================] - 21s 258ms/step - loss: 0.0693 - accuracy: 0.9821\n",
      "Epoch 13/20\n",
      "82/82 [==============================] - 23s 280ms/step - loss: 0.0564 - accuracy: 0.9860\n",
      "Epoch 14/20\n",
      "82/82 [==============================] - 25s 305ms/step - loss: 0.0530 - accuracy: 0.9865\n",
      "Epoch 15/20\n",
      "82/82 [==============================] - 25s 303ms/step - loss: 0.0479 - accuracy: 0.9881\n",
      "Epoch 16/20\n",
      "82/82 [==============================] - 24s 290ms/step - loss: 0.0422 - accuracy: 0.9897\n",
      "Epoch 17/20\n",
      "82/82 [==============================] - 21s 253ms/step - loss: 0.0403 - accuracy: 0.9899\n",
      "Epoch 18/20\n",
      "82/82 [==============================] - 20s 239ms/step - loss: 0.0367 - accuracy: 0.9907\n",
      "Epoch 19/20\n",
      "82/82 [==============================] - 22s 267ms/step - loss: 0.0328 - accuracy: 0.9918\n",
      "Epoch 20/20\n",
      "82/82 [==============================] - 21s 252ms/step - loss: 0.0305 - accuracy: 0.9923\n",
      "Score for fold 5: loss of 0.1580973118543625; accuracy of 96.59120440483093%\n"
     ]
    }
   ],
   "source": [
    "cnnbilstm_acc, cnnbilstm_loss, cnnbilstm_f1, cnnbilstm_reports = cnnbilstm(x,y,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7226158968667293,\n",
       " 0.7424922333448394,\n",
       " 0.7192607170360229,\n",
       " 0.6994845360824743,\n",
       " 0.7120596205962059]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnnbilstm_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7191826007852544"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cnnbilstm_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014098816929111881"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(cnnbilstm_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
